{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "import re\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8MoOTx-T994",
        "outputId": "88d70cd0-b4ac-440c-8a15-6c2ff13e9fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEAd5eW0tGPO",
        "outputId": "b46804a8-1005-4adb-f5e3-dbaac9c9f646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install keybert pandas numpy PyMuPDF scikit-learn sentence-transformers\n",
        "\n",
        "import fitz\n",
        "import numpy as np\n",
        "from keybert import KeyBERT\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.cluster import KMeans\n",
        "import logging\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7EJmc68TG7u",
        "outputId": "807aa72c-9576-4344-e2b2-9647132b2dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.14)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: keybert\n",
            "Successfully installed keybert-0.8.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdgMz-Hdu8IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "HBRMrz1nuVM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyHeaderExtractor:\n",
        "    def __init__(self):\n",
        "        self.kw_model = KeyBERT()\n",
        "\n",
        "    def extract_document_structure(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Enhanced document structure extraction with better spacing detection.\"\"\"\n",
        "        formatted_blocks = []\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            prev_y1 = None\n",
        "            prev_font_size = None\n",
        "            page_heights = []\n",
        "\n",
        "            # First pass - collect page heights and overall stats\n",
        "            for page in doc:\n",
        "                page_heights.append(page.rect.height)\n",
        "\n",
        "            avg_page_height = np.mean(page_heights)\n",
        "\n",
        "            for page_num, page in enumerate(doc):\n",
        "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "                for block in blocks:\n",
        "                    if \"lines\" in block:\n",
        "                        for line in block[\"lines\"]:\n",
        "                            y0 = line[\"bbox\"][1]\n",
        "                            # Calculate spacing from previous line\n",
        "                            line_spacing = y0 - prev_y1 if prev_y1 is not None else 0\n",
        "\n",
        "                            for span in line[\"spans\"]:\n",
        "                                text = span[\"text\"].strip()\n",
        "                                if text and len(text) > 1:  # Filter out single characters\n",
        "                                    font_size = span[\"size\"]\n",
        "\n",
        "                                    # Calculate relative position on page\n",
        "                                    relative_y = y0 / avg_page_height\n",
        "\n",
        "                                    # Detect if this is likely a header based on spacing\n",
        "                                    extra_spacing_before = line_spacing > 1.5 * font_size\n",
        "                                    font_size_change = (prev_font_size and font_size > prev_font_size)\n",
        "\n",
        "                                    block_info = {\n",
        "                                        \"text\": text,\n",
        "                                        \"font_size\": font_size,\n",
        "                                        \"font_name\": span[\"font\"],\n",
        "                                        \"is_bold\": \"bold\" in span[\"font\"].lower() or span[\"flags\"] & 2**4 != 0,\n",
        "                                        \"page_num\": page_num + 1,\n",
        "                                        \"y_position\": y0,\n",
        "                                        \"relative_y\": relative_y,\n",
        "                                        \"line_spacing\": line_spacing,\n",
        "                                        \"extra_spacing_before\": extra_spacing_before,\n",
        "                                        \"font_size_change\": font_size_change,\n",
        "                                        \"bbox\": line[\"bbox\"],\n",
        "                                        \"char_count\": len(text)\n",
        "                                    }\n",
        "                                    formatted_blocks.append(block_info)\n",
        "\n",
        "                                    prev_y1 = line[\"bbox\"][3]\n",
        "                                    prev_font_size = font_size\n",
        "\n",
        "            return formatted_blocks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF {pdf_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def identify_potential_headers(self, blocks: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Identify headers using multiple heuristics.\"\"\"\n",
        "        # Calculate document statistics\n",
        "        font_sizes = [block[\"font_size\"] for block in blocks]\n",
        "        avg_font_size = np.mean(font_sizes)\n",
        "        std_font_size = np.std(font_sizes)\n",
        "\n",
        "        headers = []\n",
        "\n",
        "        for i, block in enumerate(blocks):\n",
        "            score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # 1. Font Size Analysis\n",
        "            if block[\"font_size\"] > avg_font_size + std_font_size:\n",
        "                score += 2\n",
        "                reasons.append(\"large_font\")\n",
        "            elif block[\"font_size\"] > avg_font_size + (std_font_size * 0.5):\n",
        "                score += 1\n",
        "                reasons.append(\"medium_font\")\n",
        "\n",
        "            # 2. Bold Text\n",
        "            if block[\"is_bold\"]:\n",
        "                score += 1.5\n",
        "                reasons.append(\"bold\")\n",
        "\n",
        "            # 3. Spacing Analysis\n",
        "            if block[\"extra_spacing_before\"]:\n",
        "                score += 1\n",
        "                reasons.append(\"extra_spacing\")\n",
        "\n",
        "            # 4. Text Length\n",
        "            words = block[\"text\"].split()\n",
        "            if len(words) <= 8:\n",
        "                score += 1\n",
        "                reasons.append(\"concise\")\n",
        "\n",
        "            # 5. Text Case\n",
        "            if block[\"text\"].istitle() or block[\"text\"].isupper():\n",
        "                score += 0.5\n",
        "                reasons.append(\"title_case\")\n",
        "\n",
        "            # 6. Position on Page\n",
        "            if block[\"relative_y\"] < 0.2:  # Near top of page\n",
        "                score += 0.5\n",
        "                reasons.append(\"top_position\")\n",
        "\n",
        "            # 7. Common Header Terms\n",
        "            header_terms = [\n",
        "                'privacy', 'information', 'data', 'rights', 'policy',\n",
        "                'collect', 'use', 'share', 'protect', 'security',\n",
        "                'contact', 'changes', 'cookie', 'personal', 'agreement'\n",
        "            ]\n",
        "\n",
        "            if any(term in block[\"text\"].lower() for term in header_terms):\n",
        "                score += 1\n",
        "                reasons.append(\"header_term\")\n",
        "\n",
        "            # 8. Context Analysis\n",
        "            if i > 0 and i < len(blocks) - 1:\n",
        "                # Check if different formatting from surrounding text\n",
        "                if (block[\"font_size\"] > blocks[i-1][\"font_size\"] and\n",
        "                    (i == len(blocks)-1 or block[\"font_size\"] > blocks[i+1][\"font_size\"])):\n",
        "                    score += 1\n",
        "                    reasons.append(\"format_break\")\n",
        "\n",
        "            if score >= 2.5:  # Adjusted threshold\n",
        "                headers.append({\n",
        "                    \"text\": block[\"text\"],\n",
        "                    \"page_num\": block[\"page_num\"],\n",
        "                    \"score\": score,\n",
        "                    \"reasons\": reasons,\n",
        "                    \"y_position\": block[\"y_position\"],\n",
        "                    \"font_size\": block[\"font_size\"]\n",
        "                })\n",
        "\n",
        "        return headers\n",
        "\n",
        "    def process_policy(self, pdf_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Process a single policy document.\"\"\"\n",
        "        blocks = self.extract_document_structure(pdf_path)\n",
        "        headers = self.identify_potential_headers(blocks)\n",
        "\n",
        "        # Convert to DataFrame and sort\n",
        "        df = pd.DataFrame(headers)\n",
        "        if not df.empty:\n",
        "            df = df.sort_values([\"page_num\", \"y_position\"])\n",
        "\n",
        "            # Remove near-duplicate headers\n",
        "            df = df.drop_duplicates(subset=['text'], keep='first')\n",
        "\n",
        "            # Filter out likely false positives\n",
        "            df = df[~df['text'].str.contains(r'^[\\d\\W]+$')]  # Remove numeric/symbol only\n",
        "            df = df[df['text'].str.len() > 3]  # Remove very short text\n",
        "\n",
        "        return df\n",
        "\n"
      ],
      "metadata": {
        "id": "WwDVPy05W4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_potential_header(self, block: Dict, doc_stats: Dict) -> Dict:\n",
        "    \"\"\"Enhanced scoring system for header detection.\"\"\"\n",
        "    text = block[\"text\"]\n",
        "\n",
        "    # Format scoring - Relaxed criteria\n",
        "    format_score = 0\n",
        "    format_reasons = []\n",
        "\n",
        "    # Font size scoring - Lower thresholds\n",
        "    if block[\"font_size_category\"] == \"largest\":\n",
        "        format_score += 2.0\n",
        "        format_reasons.append(\"largest_font\")\n",
        "    elif block[\"font_size_category\"] == \"large\":\n",
        "        format_score += 1.5  # Increased from 1.0\n",
        "        format_reasons.append(\"large_font\")\n",
        "\n",
        "    # Bold text - Same weight\n",
        "    if block[\"is_bold\"]:\n",
        "        format_score += 1.0\n",
        "        format_reasons.append(\"bold\")\n",
        "\n",
        "    # Text length - More permissive\n",
        "    words = text.split()\n",
        "    if len(words) <= 8:  # Increased from 6\n",
        "        format_score += 0.5\n",
        "        format_reasons.append(\"concise\")\n",
        "    elif len(words) <= 12:  # Added medium length\n",
        "        format_score += 0.3\n",
        "        format_reasons.append(\"medium_length\")\n",
        "\n",
        "    # Capitalization - Same weights\n",
        "    if text.istitle():\n",
        "        format_score += 0.5\n",
        "        format_reasons.append(\"title_case\")\n",
        "    elif text.isupper():\n",
        "        format_score += 0.3\n",
        "        format_reasons.append(\"all_caps\")\n",
        "\n",
        "    # New line/paragraph\n",
        "    if block[\"new_line\"]:\n",
        "        format_score += 0.5\n",
        "        format_reasons.append(\"new_line\")\n",
        "\n",
        "    # Common header patterns\n",
        "    header_patterns = [\n",
        "        r\"^(\\d+\\.)+\\s\",  # Numbered sections like \"1.1\", \"2.3.1\"\n",
        "        r\"^[A-Z]\\.\\s\",   # Letter sections like \"A.\", \"B.\"\n",
        "        r\"how\\s+we\",     # Common privacy policy phrases\n",
        "        r\"your\\s+rights\",\n",
        "        r\"information\\s+we\",\n",
        "        r\"personal\\s+information\",\n",
        "        r\"data\\s+[a-z]+ing\",  # data sharing, data processing etc\n",
        "        r\"privacy\",\n",
        "        r\"security\",\n",
        "        r\"changes\",\n",
        "        r\"contact\",\n",
        "        r\"cookies?\"\n",
        "    ]\n",
        "\n",
        "    for pattern in header_patterns:\n",
        "        if re.search(pattern, text.lower()):\n",
        "            format_score += 0.5\n",
        "            format_reasons.append(\"header_pattern\")\n",
        "            break\n",
        "\n",
        "    # Content scoring using KeyBERT with expanded privacy-related terms\n",
        "    privacy_terms = [\n",
        "        \"privacy\", \"data\", \"information\", \"rights\", \"security\",\n",
        "        \"collect\", \"process\", \"share\", \"protect\", \"store\",\n",
        "        \"retain\", \"delete\", \"transfer\", \"disclosure\", \"consent\",\n",
        "        \"cookie\", \"access\", \"control\", \"opt\", \"choice\"\n",
        "    ]\n",
        "\n",
        "    keywords = self.kw_model.extract_keywords(\n",
        "        text,\n",
        "        keyphrase_ngram_range=(1, 3),\n",
        "        stop_words=\"english\",\n",
        "        use_maxsum=True,\n",
        "        top_n=5  # Increased from 3\n",
        "    )\n",
        "\n",
        "    # Boost scores for privacy-related terms\n",
        "    content_score = sum(score * 1.5 if any(term in kw.lower() for term in privacy_terms) else score\n",
        "                       for kw, score in keywords)\n",
        "    content_terms = [term for term, _ in keywords]\n",
        "\n",
        "    # Calculate total score with adjusted weights\n",
        "    # Increased weight for format score since it's more reliable for headers\n",
        "    total_score = (format_score * 0.75) + (content_score * 0.25)\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"total_score\": total_score,\n",
        "        \"format_score\": format_score,\n",
        "        \"content_score\": content_score,\n",
        "        \"format_reasons\": format_reasons,\n",
        "        \"content_terms\": content_terms,\n",
        "        \"page_num\": block[\"page_num\"],\n",
        "        \"y_position\": block[\"y_position\"]\n",
        "    }\n",
        "\n",
        "# Update extract_headers with lower threshold\n",
        "def extract_headers(self, pdf_path: str, min_score: float = 1.5) -> pd.DataFrame:  # Lowered from 2.0\n",
        "    \"\"\"Extract headers with more permissive threshold.\"\"\"\n",
        "    blocks = self.extract_document_structure(pdf_path)\n",
        "    if not blocks:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    doc_stats = self.analyze_font_sizes([b[\"font_size\"] for b in blocks])\n",
        "\n",
        "    headers = []\n",
        "    for block in blocks:\n",
        "        scored = self.score_potential_header(block, doc_stats)\n",
        "        if scored[\"total_score\"] >= min_score:\n",
        "            headers.append(scored)\n",
        "\n",
        "    # Convert to DataFrame and sort\n",
        "    df = pd.DataFrame(headers)\n",
        "    if not df.empty:\n",
        "        df = df.sort_values([\"page_num\", \"y_position\"])\n",
        "        # Remove near-duplicate headers\n",
        "        df = df.drop_duplicates(subset=['text'], keep='first')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZDiKgwChaYgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_multiple_policies(pdf_paths: List[str]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Process multiple policies.\"\"\"\n",
        "    extractor = PolicyHeaderExtractor()\n",
        "    results = {}\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        print(f\"\\nProcessing: {pdf_path}\")\n",
        "        try:\n",
        "            headers_df = extractor.process_policy(pdf_path)\n",
        "            results[pdf_path] = headers_df\n",
        "\n",
        "            if not headers_df.empty:\n",
        "                print(f\"\\nFound {len(headers_df)} headers:\")\n",
        "                display_df = headers_df.sort_values('score', ascending=False)\n",
        "                print(display_df[['text', 'score', 'reasons', 'page_num']].head(10).to_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # List of policy PDFs to process\n",
        "    policy_paths = [\n",
        "        \"/content/drive/MyDrive/210 Capstone/moonpay.pdf\",\n",
        "        # \"/content/drive/MyDrive/210 Capstone/stripe.pdf\",\n",
        "        # \"/content/drive/MyDrive/210 Capstone/klarna.pdf\"\n",
        "        # Add other policy paths\n",
        "    ]\n",
        "\n",
        "    results = process_multiple_policies(policy_paths)\n",
        "\n",
        "    # Save results to Excel with multiple sheets\n",
        "    with pd.ExcelWriter('policy_headers_analysis.xlsx') as writer:\n",
        "        for pdf_path, df in results.items():\n",
        "            sheet_name = pdf_path.split('/')[-1].replace('.pdf', '')[:31]  # Excel limits sheet names to 31 chars\n",
        "            if not df.empty:\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o2e6BOTZAL3",
        "outputId": "c239aed2-a854-4c26-d15b-ff76ff0a4fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/210 Capstone/moonpay.pdf\n",
            "\n",
            "Found 80 headers:\n",
            "                                              text  score                                                                            reasons  page_num\n",
            "2                            Global Privacy Policy    8.0  [large_font, bold, extra_spacing, concise, title_case, header_term, format_break]         1\n",
            "63                               How to contact us    6.5      [large_font, extra_spacing, concise, top_position, header_term, format_break]        13\n",
            "103                    How to exercise your rights    6.5      [large_font, extra_spacing, concise, top_position, header_term, format_break]        19\n",
            "105                         Vermont Privacy Rights    6.5        [large_font, extra_spacing, concise, title_case, header_term, format_break]        19\n",
            "59                             Your privacy rights    6.5      [large_font, extra_spacing, concise, top_position, header_term, format_break]        12\n",
            "14                 Personal Information we collect    6.5      [large_font, extra_spacing, concise, top_position, header_term, format_break]         3\n",
            "90                       California Privacy Rights    6.5        [large_font, extra_spacing, concise, title_case, header_term, format_break]        16\n",
            "51                 Children's personal information    6.0                    [large_font, extra_spacing, concise, header_term, format_break]        10\n",
            "45   How we protect and store personal information    6.0                    [large_font, extra_spacing, concise, header_term, format_break]         9\n",
            "55            Data transferred out of the EU or UK    6.0                    [large_font, extra_spacing, concise, header_term, format_break]        11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyHeaderExtractorBalanced:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_document_structure(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extracts and organizes document structure based on text blocks.\"\"\"\n",
        "        formatted_blocks = []\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            prev_y1 = None\n",
        "            page_heights = []\n",
        "\n",
        "            for page in doc:\n",
        "                page_heights.append(page.rect.height)\n",
        "\n",
        "            avg_page_height = np.mean(page_heights)\n",
        "\n",
        "            for page_num, page in enumerate(doc):\n",
        "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "                for block in blocks:\n",
        "                    if \"lines\" in block:\n",
        "                        for line in block[\"lines\"]:\n",
        "                            y0 = line[\"bbox\"][1]\n",
        "                            line_spacing = y0 - prev_y1 if prev_y1 is not None else 0\n",
        "\n",
        "                            for span in line[\"spans\"]:\n",
        "                                text = span[\"text\"].strip()\n",
        "                                if text and len(text) > 1:  # Ignore single characters\n",
        "                                    font_size = span[\"size\"]\n",
        "                                    relative_y = y0 / avg_page_height\n",
        "\n",
        "                                    formatted_blocks.append({\n",
        "                                        \"text\": text,\n",
        "                                        \"font_size\": font_size,\n",
        "                                        \"is_bold\": \"bold\" in span[\"font\"].lower() or span[\"flags\"] & 2**4 != 0,\n",
        "                                        \"page_num\": page_num + 1,\n",
        "                                        \"relative_y\": relative_y,\n",
        "                                        \"line_spacing\": line_spacing,\n",
        "                                        \"char_count\": len(text),\n",
        "                                    })\n",
        "                                    prev_y1 = line[\"bbox\"][3]\n",
        "\n",
        "            return formatted_blocks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF {pdf_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def identify_potential_headers(self, blocks: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Identifies potential headers with balanced heuristics.\"\"\"\n",
        "        headers = []\n",
        "\n",
        "        for block in blocks:\n",
        "            # Re-include potential section headers near the top of the first page\n",
        "            if block[\"page_num\"] == 1 and block[\"relative_y\"] < 0.15 and block[\"font_size\"] > 12:\n",
        "                pass  # Keep this block\n",
        "\n",
        "            # Skip generic footers (only repeated elements or positioned at the bottom)\n",
        "            if block[\"relative_y\"] > 0.9 and \"copyright\" in block[\"text\"].lower():\n",
        "                continue\n",
        "\n",
        "            # Ensure meaningful content\n",
        "            if block[\"char_count\"] <= 3 or block[\"text\"].isdigit():\n",
        "                continue\n",
        "\n",
        "            # Heuristic scoring\n",
        "            score = 0\n",
        "            if block[\"is_bold\"]:\n",
        "                score += 1.5\n",
        "            if block[\"font_size\"] > 12:  # Arbitrary threshold for larger fonts\n",
        "                score += 1\n",
        "            if 3 <= len(block[\"text\"].split()) <= 8:\n",
        "                score += 1\n",
        "            if block[\"relative_y\"] < 0.2:  # Prioritize top sections\n",
        "                score += 0.5\n",
        "\n",
        "            # Add as a header if it meets a more balanced score threshold\n",
        "            if score >= 2.0:  # Lower threshold slightly\n",
        "                headers.append({\n",
        "                    \"text\": block[\"text\"],\n",
        "                    \"page_num\": block[\"page_num\"],\n",
        "                    \"font_size\": block[\"font_size\"],\n",
        "                    \"relative_y\": block[\"relative_y\"],\n",
        "                    \"score\": score,\n",
        "                })\n",
        "\n",
        "        return headers\n",
        "\n",
        "    def process_policy(self, pdf_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Processes a policy document to extract headers.\"\"\"\n",
        "        blocks = self.extract_document_structure(pdf_path)\n",
        "        headers = self.identify_potential_headers(blocks)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(headers)\n",
        "        if not df.empty:\n",
        "            df = df.sort_values([\"page_num\", \"relative_y\"])\n",
        "            df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfsSfU5rZBVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of policy PDFs to process\n",
        "policy_paths = [\n",
        "    \"/content/drive/MyDrive/210 Capstone/moonpay.pdf\",\n",
        "    \"/content/drive/MyDrive/210 Capstone/stripe.pdf\",\n",
        "    \"/content/drive/MyDrive/210 Capstone/klarna.pdf\",\n",
        "    \"/content/drive/MyDrive/210 Capstone/plaid.pdf\",\n",
        "\n",
        "    # Add other policy paths\n",
        "]\n",
        "\n",
        "# Re-run with balanced refinements\n",
        "extractor = PolicyHeaderExtractorBalanced()\n",
        "results = {}\n",
        "for pdf in policy_paths:\n",
        "    headers_df = extractor.process_policy(pdf)\n",
        "    results[pdf] = headers_df\n",
        "\n",
        "# Save results to a new file\n",
        "output_path = \"/content/drive/MyDrive/210 Capstone/policy_headers.xlsx\"\n",
        "with pd.ExcelWriter(output_path) as writer:\n",
        "    for pdf, df in results.items():\n",
        "        sheet_name = pdf.split(\"/\")[-1].replace(\".pdf\", \"\")\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "bQPMcjiUjO9T",
        "outputId": "f7de065f-fec6-41a2-f8d3-e48d88a4389d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing PDF /content/drive/MyDrive/210 Capstone/plaid.pdf: no such file: '/content/drive/MyDrive/210 Capstone/plaid.pdf'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/210 Capstone/policy_headers.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahSiZJFcn2Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    pdf_file_obj = open(file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    text = ''\n",
        "    for page in range(num_pages):\n",
        "        page_obj = pdf_reader.pages[page]\n",
        "        text += page_obj.extract_text()\n",
        "    pdf_file_obj.close()\n",
        "    return text\n",
        "\n",
        "def extract_section_headers(text: str) -> List[Dict]:\n",
        "    \"\"\"Extracts section headers from the given text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    section_headers = []\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if token.pos_ == \"PROPN\" and token.dep_ == \"ROOT\":\n",
        "                section_header = sent.text.strip()\n",
        "                section_headers.append({\"text\": section_header})\n",
        "    return section_headers\n",
        "\n",
        "def process_pdfs_in_directory(directory_path: str) -> Dict:\n",
        "    \"\"\"Processes PDF files in the given directory.\"\"\"\n",
        "    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
        "    results = {}\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(directory_path, pdf_file)\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        section_headers = extract_section_headers(text)\n",
        "        results[pdf_file] = section_headers\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/210 Capstone/\"\n",
        "results = process_pdfs_in_directory(directory_path)\n",
        "\n",
        "for pdf_file, section_headers in results.items():\n",
        "    print(f\"Section Headers for {pdf_file}:\")\n",
        "    df = pd.DataFrame(section_headers)\n",
        "    print(df)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfh6QisUriEh",
        "outputId": "eac23c56-8bd6-42ac-83b4-47a82d7e799f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section Headers for stripe.pdf:\n",
            "                                                 text\n",
            "0   “Sites”refer to Stripe.com, Link.com, and othe...\n",
            "1   Personal Data that we collect and how we use a...\n",
            "2                                       Contact us10.\n",
            "3                        US Consumer Privacy Notice1.\n",
            "4   Personal Data we collect and how weuse and sha...\n",
            "5   Personal Data we collect about End UsersUsing ...\n",
            "6                     Identity/Verification Services.\n",
            "7                                 Our Business Users.\n",
            "8                Fraud Detection and Loss Prevention.\n",
            "9   Personal Data we collect about End CustomersTr...\n",
            "10                 Identity/Verification Information.\n",
            "11  T o protect against fraud and determine if som...\n",
            "12                                                  s\n",
            "13                                         i c e s  t\n",
            "14   En d  C u s t o m e rs 'Personal Data with them.\n",
            "15  The Business User you choose to do business wi...\n",
            "16  Our Business Users (and their authorized third...\n",
            "17  Personal Data we collect about Representatives...\n",
            "18                        Identification Information.\n",
            "19                                 Business Services.\n",
            "20                      Forums and Discussion Groups.\n",
            "21                              Use of Personal Data.\n",
            "22                                         LearnMore.\n",
            "23                       Social Media and Promotions.\n",
            "24                     Fraud Prevention and Security.\n",
            "25                   Service Providers or Processors.\n",
            "26                                Financial Partners.\n",
            "27                            Corporate Transactions.\n",
            "28  Contractual and Pre-Contractual Business Relat...\n",
            "29                                  Legal Compliance.\n",
            "30                              Legitimate Interests.\n",
            "31                       Substantial Public Interest.\n",
            "32                                      a  p ro t e c\n",
            "33                                                  t\n",
            "34                                           g h t s,\n",
            "35  i t  o u r  Privacy Center or contact us as ou...\n",
            "36  Privacy Policyhttps://stripe.com/privacy\\n22 o...\n",
            "37                                            Brazil.\n",
            "38  P-6.5,in Alberta, and the Act Respecting the P...\n",
            "39                                        EEA and UK.\n",
            "40                                             India.\n",
            "41                                         Indonesia.\n",
            "42                                             Japan.\n",
            "43                                         Singapore.\n",
            "44                                       Switzerland.\n",
            "45                                          Thailand.\n",
            "46                                     United States.\n",
            "47                                                  g\n",
            "48                                                  h\n",
            "49                                           i c e s.\n",
            "50                            i t  a  re q u e s t  t\n",
            "51                                                g h\n",
            "52                                             t s  d\n",
            "53  e s c r i b e d  a b ove,  p l e a s e  c o n ...\n",
            "54                                        California:\n",
            "55  Method TermsUser Bank Debit AuthorizationsProh...\n",
            "\n",
            "Section Headers for klarna.pdf:\n",
            "                                                  text\n",
            "0    Contact\\nand\\nidentiﬁcation\\ndata - Name, date...\n",
            "1    Information\\nabout\\ngoods/services - Details c...\n",
            "2    Information\\nabout\\nyour\\ncontacts\\nwith\\nKlar...\n",
            "3    Device\\ninformation - Device ID, IP address, l...\n",
            "4    Klarna will also\\nprocess data about third par...\n",
            "..                                                 ...\n",
            "124  Description\\nof\\nthe\\nrecipient:\\nLogistics an...\n",
            "125  Description\\nof\\nthe\\nrecipient: Third party\\n...\n",
            "126  Purpose\\nand\\nlegal\\nbasis:\\nDone to fulﬁl the...\n",
            "127  7.6\\nCategories\\nof\\nrecipients\\nwith\\nwhich\\n...\n",
            "128  Description\\nof\\nthe\\nrecipient:\\nSocial media...\n",
            "\n",
            "[129 rows x 1 columns]\n",
            "\n",
            "Section Headers for plaid.pdf:\n",
            "                                                text\n",
            "0  Some Final Details…  • International Data Tran...\n",
            "1  Develop Existing Services:  To improve, enhanc...\n",
            "2  • Develop New Services:  To develop new produc...\n",
            "3  Provide Support:  To provide support to you or...\n",
            "4  Communicate With You: To communicate with you ...\n",
            "5  Our Lawful Bases for Processing (EEA and UK En...\n",
            "6  Consumer Privacy Notice  \\nLast Updated: Febru...\n",
            "7                                         Plaid Inc.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBeiwAlErt6G",
        "outputId": "7a720879-f462-4894-8bf7-9c6111d43551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0  Some Final Details…  • International Data Tran...\n",
            "1  Develop Existing Services:  To improve, enhanc...\n",
            "2  • Develop New Services:  To develop new produc...\n",
            "3  Provide Support:  To provide support to you or...\n",
            "4  Communicate With You: To communicate with you ...\n",
            "5  Our Lawful Bases for Processing (EEA and UK En...\n",
            "6  Consumer Privacy Notice  \\nLast Updated: Febru...\n",
            "7                                         Plaid Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    pdf_file_obj = open(file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    text = ''\n",
        "    for page in range(num_pages):\n",
        "        page_obj = pdf_reader.pages[page]\n",
        "        text += page_obj.extract_text()\n",
        "    pdf_file_obj.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_section_headers(text: str) -> List[Dict]:\n",
        "    \"\"\"Extracts section headers from the given text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    section_headers = []\n",
        "\n",
        "    # Patterns to identify headers\n",
        "    common_header_keywords = [\n",
        "        \"privacy\", \"policy\", \"data\", \"information\", \"rights\", \"contact\", \"changes\",\n",
        "        \"security\", \"collect\", \"use\", \"share\", \"protect\", \"cookie\", \"terms\", \"personal\"\n",
        "    ]\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        text = sent.text.strip()\n",
        "\n",
        "        # Skip overly short sentences or purely numeric content\n",
        "        if len(text) < 5 or text.isdigit():\n",
        "            continue\n",
        "\n",
        "        # Check for header-like features\n",
        "        if (\n",
        "            text.istitle() or text.isupper() or any(word.lower() in text.lower() for word in common_header_keywords)\n",
        "        ) and len(text.split()) <= 10:  # Limit to concise headers\n",
        "            section_headers.append({\"text\": text})\n",
        "\n",
        "    return section_headers\n",
        "\n",
        "\n",
        "def process_pdfs_in_directory(directory_path: str) -> Dict:\n",
        "    \"\"\"Processes PDF files in the given directory.\"\"\"\n",
        "    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
        "    results = {}\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(directory_path, pdf_file)\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        section_headers = extract_section_headers(text)\n",
        "        results[pdf_file] = section_headers\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/210 Capstone/policy\"\n",
        "results = process_pdfs_in_directory(directory_path)\n",
        "\n",
        "# Save results to Excel\n",
        "with pd.ExcelWriter(f\"{directory_path}/section_headers_output.xlsx\") as writer:\n",
        "    for pdf_file, section_headers in results.items():\n",
        "        df = pd.DataFrame(section_headers)\n",
        "        sheet_name = pdf_file.split('.')[0][:31]  # Sheet names must be <= 31 chars\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "Q3DWlzydryPE",
        "outputId": "4189303e-1b9d-4041-da99-90591b3222c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/pdf/directory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5da2ffb9a3cf>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mdirectory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/pdf/directory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_pdfs_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Save results to an Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-5da2ffb9a3cf>\u001b[0m in \u001b[0;36mprocess_pdfs_in_directory\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pdfs_in_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m\"\"\"Processes all PDFs in a directory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mpdf_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/pdf/directory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    with open(file_path, 'rb') as pdf_file_obj:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def is_potential_header(text: str) -> bool:\n",
        "    \"\"\"Determine if a given line is likely a section header.\"\"\"\n",
        "    # Common header keywords\n",
        "    header_keywords = [\n",
        "        \"privacy\", \"policy\", \"data\", \"information\", \"rights\", \"contact\", \"security\",\n",
        "        \"collect\", \"use\", \"share\", \"protect\", \"cookie\", \"terms\", \"personal\"\n",
        "    ]\n",
        "\n",
        "    # Basic filtering criteria\n",
        "    if len(text.split()) > 10:  # Headers should be short\n",
        "        return False\n",
        "    if re.match(r\"^[\\d\\s\\W]+$\", text):  # Ignore numeric/symbolic-only lines\n",
        "        return False\n",
        "    if \"copyright\" in text.lower() or \"protected by\" in text.lower():  # Ignore legal notices\n",
        "        return False\n",
        "    if any(word in text.lower() for word in header_keywords):  # Check for key terms\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def extract_section_headers(text: str) -> List[Dict]:\n",
        "    \"\"\"Extracts section headers from the given text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    section_headers = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        text = sent.text.strip()\n",
        "        if is_potential_header(text):  # Apply filtering\n",
        "            section_headers.append({\"text\": text})\n",
        "    return section_headers\n",
        "\n",
        "\n",
        "def process_pdf(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Processes a single PDF to extract headers.\"\"\"\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "    headers = extract_section_headers(text)\n",
        "    return pd.DataFrame(headers)\n",
        "\n",
        "\n",
        "def process_pdfs_in_directory(directory_path: str) -> Dict:\n",
        "    \"\"\"Processes all PDFs in a directory.\"\"\"\n",
        "    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
        "    results = {}\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(directory_path, pdf_file)\n",
        "        headers_df = process_pdf(file_path)\n",
        "        results[pdf_file] = headers_df\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/210 Capstone/policy/\"\n",
        "results = process_pdfs_in_directory(directory_path)\n",
        "\n",
        "# Save results to an Excel file\n",
        "output_path = \"/content/drive/MyDrive/210 Capstone/section_headers_output.xlsx\"\n",
        "with pd.ExcelWriter(output_path) as writer:\n",
        "    for pdf_file, headers_df in results.items():\n",
        "        sheet_name = pdf_file.split('.')[0][:31]  # Excel sheet name limit\n",
        "        headers_df.to_excel(writer, sheet_name=sheet_name, index=False)"
      ],
      "metadata": {
        "id": "LR0Cd2BUwVZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extracts raw text from a PDF.\"\"\"\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def is_potential_header(text: str, position_on_page: float = None) -> bool:\n",
        "    \"\"\"Determine if a line is a valid section header.\"\"\"\n",
        "    header_keywords = [\n",
        "        \"privacy\", \"policy\", \"data\", \"information\", \"rights\", \"contact\", \"security\",\n",
        "        \"collect\", \"use\", \"share\", \"protect\", \"cookie\", \"terms\", \"personal\"\n",
        "    ]\n",
        "    if len(text.split()) > 10:  # Too long to be a header\n",
        "        return False\n",
        "    if re.match(r\"^[\\d\\s\\W]+$\", text):  # Purely numeric or special characters\n",
        "        return False\n",
        "    if \"copyright\" in text.lower() or \"https://\" in text.lower():  # Footers and URLs\n",
        "        return False\n",
        "    if position_on_page is not None and position_on_page < 0.1:  # Likely document title\n",
        "        return False\n",
        "    return any(word in text.lower() for word in header_keywords)\n",
        "\n",
        "\n",
        "def extract_section_headers(text: str) -> List[Dict]:\n",
        "    \"\"\"Extract section headers from text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    headers = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        text = sent.text.strip()\n",
        "        # Use basic position-based filtering\n",
        "        position_on_page = len(text) / len(doc.text)  # Approximation\n",
        "        if is_potential_header(text, position_on_page=position_on_page):\n",
        "            headers.append({\"text\": text})\n",
        "    return headers\n",
        "\n",
        "\n",
        "def process_pdf(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Process a single PDF to extract section headers.\"\"\"\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "    headers = extract_section_headers(text)\n",
        "    return pd.DataFrame(headers)"
      ],
      "metadata": {
        "id": "_NqlCKXoxVyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "file_path = \"/content/drive/MyDrive/210 Capstone/policy/moonpay.pdf\"  # Replace with the correct path\n",
        "headers_df = process_pdf(file_path)\n",
        "print(headers_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfjogK9eyF0s",
        "outputId": "55f053d2-68e9-4ab2-9640-0e68aa978d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF"
      ],
      "metadata": {
        "id": "uFiasKIp1K2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyHeaderExtractor:\n",
        "    def __init__(self):\n",
        "        self.keywords = [\"privacy\", \"information\", \"data\", \"rights\", \"policy\", \"cookie\", \"contact\", \"security\"]\n",
        "\n",
        "    def extract_document_structure(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extract text and layout details.\"\"\"\n",
        "        blocks = []\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page_num, page in enumerate(doc):\n",
        "                for block in page.get_text(\"dict\")[\"blocks\"]:\n",
        "                    for line in block.get(\"lines\", []):\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text = span[\"text\"].strip()\n",
        "                            if text:\n",
        "                                blocks.append({\n",
        "                                    \"text\": text,\n",
        "                                    \"font_size\": span[\"size\"],\n",
        "                                    \"is_bold\": \"bold\" in span[\"font\"].lower(),\n",
        "                                    \"page_num\": page_num + 1,\n",
        "                                    \"y_position\": line[\"bbox\"][1],\n",
        "                                    \"char_count\": len(text)\n",
        "                                })\n",
        "            return blocks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def identify_headers(self, blocks: List[Dict]) -> pd.DataFrame:\n",
        "        \"\"\"Identify potential headers.\"\"\"\n",
        "        headers = []\n",
        "        for block in blocks:\n",
        "            score = 0\n",
        "            # Prioritize layout features\n",
        "            if block[\"font_size\"] > 12:\n",
        "                score += 1\n",
        "            if block[\"is_bold\"]:\n",
        "                score += 1\n",
        "            if len(block[\"text\"].split()) <= 8:\n",
        "                score += 0.5\n",
        "            if block[\"page_num\"] == 1 and block[\"y_position\"] < 200:\n",
        "                score += 0.5\n",
        "            # Include domain-specific terms\n",
        "            if any(keyword in block[\"text\"].lower() for keyword in self.keywords):\n",
        "                score += 1\n",
        "\n",
        "            if score >= 2:  # Threshold for header inclusion\n",
        "                headers.append({\n",
        "                    \"text\": block[\"text\"],\n",
        "                    \"page_num\": block[\"page_num\"],\n",
        "                    \"font_size\": block[\"font_size\"],\n",
        "                    \"score\": score\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(headers)\n",
        "\n",
        "    def process_policy(self, pdf_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Process a single policy document.\"\"\"\n",
        "        blocks = self.extract_document_structure(pdf_path)\n",
        "        if not blocks:\n",
        "            return pd.DataFrame()\n",
        "        return self.identify_headers(blocks)\n",
        "\n",
        "def process_policies_in_folder(folder_path: str) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Processes all PDF files in a folder.\"\"\"\n",
        "    extractor = PolicyHeaderExtractor()\n",
        "    results = {}\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "        headers_df = extractor.process_policy(pdf_path)\n",
        "        results[pdf_file] = headers_df\n",
        "    return results\n",
        "\n",
        "# Specify the folder containing the PDFs\n",
        "folder_path = \"/content/drive/MyDrive/210 Capstone/policy/\"\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "results = process_policies_in_folder(folder_path)\n",
        "\n",
        "# Save results to an Excel file\n",
        "output_file = \"/content/drive/MyDrive/210 Capstone/policy_headers_analysis.xlsx\"\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    for pdf, df in results.items():\n",
        "        sheet_name = pdf.split('.')[0][:31]  # Excel sheet name limit\n",
        "        if not df.empty:\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "print(f\"Headers saved to {output_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5wNEFBYyKOx",
        "outputId": "b26affd3-7282-41a7-b75d-a441f357565c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing moonpay.pdf...\n",
            "Headers saved to /content/drive/MyDrive/210 Capstone/policy_headers_analysis.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ai-Gu8k1LjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}