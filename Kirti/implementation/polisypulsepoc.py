# -*- coding: utf-8 -*-
"""PolisyPulsePOC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJK3mWYv9cOLbTLpTL67F1FocJNOkJIW

### PolicyPulse

PolicyPulse:

PolicyPulse is an AI-driven system designed to streamline the privacy policy compliance process. The project leverages a Retrieval-Augmented Generation (RAG) pipeline to analyze website privacy policies (OPP Corpus)  and determine their compliance with regulations such as GDPR and CCPA. The system allows users to upload privacy policies, which are then parsed into segments. Using a combination of large language models (LLMs) and a knowledge base of regulatory documents, the system can answer compliance-related questions and flag areas where a policy may not fully adhere to the required standards.

Key features:

1. Privacy Policy Fragmentation: Uploaded policies are broken down into manageable segments that can be individually analyzed.

2. Retrieval-Augmented Generation (RAG): The system retrieves relevant GDPR/CCPA guidelines and compares them with the uploaded policy segments to assess compliance.

3. Dynamic Q&A: Users can ask specific compliance questions, and the system will provide responses based on both the policy content and regulatory requirements.

4. Compliance Reporting: PolicyPulse generates a summary of compliance status, highlighting any gaps or missing information that need to be addressed.

5. Targeted Feedback: The system offers recommendations on how to make the policy compliant with GDPR/CCPA regulations.


*  To implement a RAG system using LangChain
*  Be able to formulate metric(s) that you may want to choose as your evaluation to what degree your system replicates gold answers (labeled data) that we will provide.
* Try out various hyper-parameters and settings to see which configuration works the best (given your chosen metric)  
* Write a comprehensive evaluation, which also includes risks and limitations (and a lot more)



Base RAG components

  2.1 Text Embeddings    
  2.2 Text Chunking   
  2.3 The Vector DB & Semantic Search  
  2.4 The Language Model   
  2.5 Testing the LLM in a LangChain Chain   
  2.6. Setting up a simple RAG Chain
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip -q install git+https://github.com/huggingface/transformers
# !pip install -q datasets loralib sentencepiece
# !pip -q install bitsandbytes accelerate
# !pip -q install langchain
# !pip install einops
# !pip install faiss-gpu
# !pip install --upgrade --quiet  langchain-community chromadb bs4 qdrant-client
# !pip install langchainhub
# 
# !pip install --upgrade --quiet  wikipedia
# !pip install --upgrade --quiet  arxiv
# !pip install --upgrade --quiet  pymupdf
# 
# !pip install xmltodict
# 
# !pip install cohere
#

import torch
import os
import bs4
import json
import numpy as np
import time


from pprint import pprint

import locale

from transformers import AutoTokenizer , AutoModelForCausalLM
from transformers import pipeline, BitsAndBytesConfig

from langchain.llms import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import Qdrant
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.utils.math import cosine_similarity

from langchain_community.document_loaders import ArxivLoader
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import WikipediaLoader
from langchain_community.document_loaders import OnlinePDFLoader
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders import PubMedLoader

from langchain_community.chat_models import ChatCohere

from google.colab import userdata
from google.colab import drive
import re
drive.mount('/content/drive')

locale.getpreferredencoding = lambda: "UTF-8"
from google.colab import userdata

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install sentence_transformers
# COHERE_API_KEY = userdata.get('COHERE_API_KEY')

locale.getpreferredencoding = lambda: "UTF-8"

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install sentence_transformers

"""Add your keys from the secret store (do **NOT** print them out or leave them exposed as plaintext in your notebook!):"""

COHERE_API_KEY = userdata.get('COHERE_API_KEY')

"""## 2. Building the Components of our RAG System

Let us introduce and test the base components of our RAG system. We will largely use the Hugging Face and LangChan libraries.

### 2.1 The Embedding Model

We will need to represent text (pieces) as vectors. For this, we will use the [sentence_transformer]() architecture.



**NOTE:** The models you can use are: 'all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', and 'avsolatorio/GIST-Embedding-v0'
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# base_embeddings = HuggingFaceEmbeddings(model_name="multi-qa-mpnet-base-dot-v1")

text = "This is a test document."
query_result = base_embeddings.embed_query(text)
print(f'Embedding dimension: {len(query_result)}')

doc_result = base_embeddings.embed_documents(["Germany won the World Cup 4 times.", "This is not a test document."])
len(doc_result)

"""Do those dimensions look correct?

Now lets see if the embedding model is working as we want.  Ideally our embeddings go beyond shared words and capture the underlying meaning.
"""

#Let's see how well our embeddng model works
similarity = cosine_similarity([query_result], doc_result)[0]

similarity

"""That's how you should define your embedding models.

Next, we turn to text chunks.

### 2.2. Loading and Chunking Texts

We first need to load the documents. Here is an example:
"""

loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)

documents = loader.load()

"""We will need to split the  text in chunks that are 'suitable' as retrieval units. Let's for starters define a chunk size of 128 and have no overlap between the chunks:  

"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)
splits = text_splitter.split_documents(documents)
print('Number of splits/chunks: ', str(len(splits)))

"""Ok, so it looks like we have now many splits (chunks) from one document. Here is how you can get the content:"""

splits[39].page_content

"""Perfect. Now we have the splits and embeddings. Next, the embeddings need to be stored in a vector db.

### 2.3 Storing the Embeddings of Chunks in Vectorstores

After loading and chunking the data, we need to save the vector representations of the chunks in a vectorstore. We will use Qdrant here for simplicity. We load the splits (structured chunks) and the embeddings:
"""

# vectorstore = Qdrant.from_documents(splits,
#     base_embeddings,
#     location=":memory:",  # Local mode with in-memory storage only
#     collection_name="test",
# )
# retriever = vectorstore.as_retriever()

"""The nice thing is that the vector store also does the similarity searches for us:"""

query = "What is Chain of Thought doing?"
docs = vectorstore.similarity_search_by_vector(base_embeddings.embed_query(query)) # will rank the splits

docs

"""Looks good! We have an ordered list of documents that seem to relate to the question. That is what we need.

The last major component is the actual LLM.

### 2.4. The LLM

We will use one Open Source Model ("mistralai/Mistral-7B-Instruct-v0.1") and one Proprietery Model (Cohere) for our tests. Let's first set up the OS model:
"""

!huggingface-cli login

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# quantization_config = BitsAndBytesConfig(load_in_4bit=True,
#                                          )
# llm_mistral_model = AutoModelForCausalLM.from_pretrained(
#     "mistralai/Mistral-7B-Instruct-v0.2",
#     torch_dtype=torch.float32,
#     device_map='auto',
#     quantization_config=quantization_config
# )
# llm_mistral_tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

"""We use the model first to generate a Hugging Face pipeline. A pipeline simplifies the process of actually generating responses."""

mistral_pipe = pipeline(
    "text-generation",
    model=llm_mistral_model,
    tokenizer=llm_mistral_tokenizer,
    max_new_tokens=1000,
    temperature=0.55,
    top_p=0.9,
    do_sample=True,
    repetition_penalty=1.3
)
mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id

"""Does it work?"""

mistral_pipe("[INST]Give me a two-sentence story about an apple![/INST]")

"""Reasonable!

We will also use a Cohere model, but will create this below as part of the LangChain framework.

### 2.5 Testing the LLM in a LangChain Chain

Chains will be defined and discussed in Week 11. In short, they are convenient programmatic ways to deal with 'chains' of actions that involve LLMs. For example, a list of events like 'here is a city name. Plug that city name into prompt template, then generate a story about that city. Lastly, format the model output as a string' can be easily handled by LangChain's Chain framework. In this case, the Chain would consist of the prompt template, the LLM, and the String Formatter. The parameter (the city in this case) will be provided at run time by invocation of the Chain. Let's test that.

To use a Hugging Face model in a LangChain environment, we need to wrap the model into a LangChain pipeline object:
"""

mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)

"""Next, we need to define a template and create a corresponding prompt template that can take any questiion"""

test_llm_template = """[INST] Give me a two-sentence story about an {object}! [/INST]"""
test_llm_prompt_template = PromptTemplate(template=test_llm_template, input_variables=["object"])

"""Let's define a Chain, a static flow of actions that (usually) involve at least a definition of the variables used in the chain, one or more templates, LLM step(s) and potentially other actions. This would be a chain that declares the variable 'object' to be expected when the chain is invoked, then inserts it into the template, and passes this to our mistral model pipeline (wrapped as a LangChain object):    """

test_llm_chain_short = (
    {"object": RunnablePassthrough()}
    | test_llm_prompt_template
    | mistral_llm_lc
)

test_llm_chain_short.invoke('apple')

"""Works too. We will use this notation moving forward.

Next, how would we do this with a Cohere Chat Model instead of Mistral?
"""

cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)

"""This can be plugged straight into the Chain:"""

test_cohere_llm_chain_short = (
    {"object": RunnablePassthrough()}
    | test_llm_prompt_template
    | cohere_chat_model
)

"""Works! (Note: you may want to review the format of the template. The one we used here is the one from Mistral, and the format may or may not be optimal for Cohere.)

How can we get the output formatting under control? We can add a String Formatter to the chain:

### 2.6 Setting Up a Simple RAG Chain

For RAG, we will follow the same approach. Except... you will **later** need to change the chain to include the retrieval step.

We first do a simple test: create a RAG template that takes a question and a pre-defined context as input, and generates the answer based on the provided context:
"""

output_parser = StrOutputParser()

rag_template = """[INST] Answer the question based only on the following context:
{context}

Question: {question}
[/INST]
"""
rag_prompt_template = ChatPromptTemplate.from_template(rag_template)

base_rag_chain =(
    {"context": RunnablePassthrough(),
     "question": RunnablePassthrough()}
    | rag_prompt_template
    | mistral_llm_lc
    | output_parser
)

predefined_context = "Germany has won the World Cup 4 times."
question = "How many times did Germany win the world cup?"

resp = base_rag_chain.invoke({'context': predefined_context,
                           'question': question})
print(resp)

"""That's great. But of course, the context needs to be created in an earlier retrieval step. More precisely, the documents will be first retrieved as a list, and then they will need to be formatted into one string to pass to the LLM in the context window.

Here is a simple formatting function that can be hooked into the chain, which combines a list of chunks into one string:


"""

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

"""So how could we build a simple chain? Let's first just get the retrieval done and the formatted retrieved data and the question inserted into the prompt template:"""

rag_template = """Here is a context:\n{context} \n\nand here is a question: \n{question}"""

rag_prompt = ChatPromptTemplate.from_template(rag_template)

rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
    | rag_prompt


)

output = rag_chain.invoke('What is Chain of Thought?')

"""Ok... with some formatting... this looks good:"""

print(output.messages[0].content)

"""Let's complete the RAG Chain:"""

output_parser = StrOutputParser()

rag_template = """[INST]Please answer the question below only based on the context information provided.\n\nHere is a context:\n{context} \n\nHere is a question: \n{question}.[/INST]"""
rag_prompt = ChatPromptTemplate.from_template(rag_template)

rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
    | rag_prompt
    | mistral_llm_lc

)

rag_chain.invoke('What is Chain of Thought?')

"""### 2.What about the Cohere models?"""

cohere_rag_chain = (
    {"context": retriever | format_docs,
     "question": RunnablePassthrough()}
    | rag_prompt
    | cohere_chat_model
    | output_parser
)

cohere_rag_chain.invoke('What is Chain of Thought?')

"""Works too! Time to build the real thing and do experimentation.

## 3. The RAG Model & Experimentation

With this we can get started. First, we need to acquire the data, chunk it, vectorize it, and store the embeddings (and in this simple case also the docs) in our Qdrant vector db.


### 3.1 The Vector Database

We will start by creating our datastore, Qdrant. Usually, you would deploy the vector db as a server, but in this case let's simply put everything in memory. Also, in this case we will store not only the embeddings but the whole document in the vector store. We will seed the store with the splits from the blog post we had used before.

We will also create the retriever, which defines the way the documents are being retrieved. The retriever parameters define for example which method is used, how many docs are retrieved, etc. See [this LangChain link ](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)for more information.
"""

import os
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

def load_specific_html_files(directory_path, specific_files):
    """
    Loads and processes the specific HTML files provided, extracting text and metadata.
    """
    all_policy_pages = []
    global_doc_number = 1

    for html_file in specific_files:
        file_path = os.path.join(directory_path, html_file)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()

                soup = BeautifulSoup(html_content, 'html.parser')
                document_text = soup.get_text()

                company_name = html_file.split('_')[1].replace('.html', '')

                document_url = f"file://{file_path}"

                all_policy_pages.append({
                    'content': document_text,
                    'metadata': {
                        'doc_num': global_doc_number,
                        'doc_source': company_name,
                        'page_num': 0,
                        'source_url': document_url
                    }
                })
                global_doc_number += 1

            except Exception as e:
                print(f"Error reading file {html_file}: {e}")
        else:
            print(f"File {html_file} not found in the directory.")

    return all_policy_pages


html_directory = '/content/drive/MyDrive/compliance/OPP-115/sanitized_policies/'
specific_files = ['453_barnesandnoble.com.html', '1636_sidearmsports.com.html', '70_meredith.com.html']

all_policy_pages = load_specific_html_files(html_directory, specific_files)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = []

for policy_page in all_policy_pages:
    chunks = text_splitter.split_text(policy_page['content'])

    for idx, chunk in enumerate(chunks):
        split_data = Document(
            page_content=chunk,
            metadata={
                'split_id': idx,
                'doc_source': policy_page['metadata']['doc_source'],
                'doc_num': policy_page['metadata']['doc_num'],
                'page_num': policy_page['metadata']['page_num'],
                'source_url': policy_page['metadata']['source_url']
            }
        )
        splits.append(split_data)

qdrant_vectorstore = Qdrant.from_documents(splits,
    base_embeddings,
    location=":memory:",
    collection_name="rag_tech_db",
    force_recreate=True
)

retriever = qdrant_vectorstore.as_retriever()

print(f"Splits successfully added to Qdrant. Total chunks: {len(splits)}")

"""### 3.2 Data Acquisition, Chunking, and Vectorization

Now where we have our store we need to get the data into it. We will need to retrieve the data, create the chunks, then vectorize them, and finally store the vectors (along with the docs in this case) in the vector db.

Let us first set chunk size and overlap, as well as the type of splitter. These are starting parameters and you may want to experiment with them:
"""

CHUNK_SIZE=1000
OVERLAP=50

text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)

"""Now let's work with an actual document collection.  We will work with four types of documents:

* A few papers from the ArXiv on RAG and NLP
* A few blogs from Lily Weng that talk about Open Domain Question Answering and related topics
* A number of Wikipedia articles on that topic

To make testing easier  we'll define a global record number so we can trace back to see which chunk came from which specific document.

"""

global_doc_number = 1

"""### Get HTML documents


"""

import re
from bs4 import BeautifulSoup
import os

def extract_company_name_from_html(html_content, html_file):
    soup = BeautifulSoup(html_content, 'html.parser')

    title_tag = soup.find('title')
    if title_tag:
        return title_tag.get_text(strip=True)

    h1_tag = soup.find('h1')
    if h1_tag:
        return h1_tag.get_text(strip=True)

    text_content = soup.get_text()
    company_name_match = re.search(r"(provided by|policy by|offered by)\s+([A-Z][a-zA-Z\s]+)", text_content)
    if company_name_match:
        return company_name_match.group(2)

    if "barnesandnoble" in html_file:
        return "Barnes & Noble"
    elif "sidearmsports" in html_file:
        return "SIDEARM Sports and its advertisers"
    elif "meredith" in html_file:
        return "meredith"

    return "Unknown Company"

def user_uploads_policy(html_file):
    full_path = os.path.join(html_path, html_file)

    with open(full_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    company_name = extract_company_name_from_html(html_content, html_file)

    return full_path, company_name, html_content

html_path = '/content/drive/MyDrive/compliance/OPP-115/sanitized_policies/'

uploaded_policy_file = '70_meredith.com.html'
policy_path, company_name, html_content = user_uploads_policy(uploaded_policy_file)

print(f"User uploaded policy: {uploaded_policy_file}")
print(f"Extracted company name: {company_name}")

all_policy_pages = []
global_doc_number = 1

soup = BeautifulSoup(html_content, 'html.parser')
document_text = soup.get_text()

all_policy_pages.append({'content': document_text, 'metadata': {'page_num': 0, 'doc_num': global_doc_number, 'doc_source': company_name}})

global_doc_number += 1

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = text_splitter.split_text(all_policy_pages[0]['content'])

for idx, split in enumerate(splits):
    splits[idx] = {'content': split, 'metadata': {'split_id': idx, 'doc_source': company_name}}
    print(f"Chunk {idx} metadata: {splits[idx]['metadata']}")

print('Number of splits/chunks: ', len(splits))

query = "What does the policy say about third-party data sharing?"


found_docs = [{'content': "We share data with third parties under certain circumstances.", 'metadata': {'doc_source': company_name}}, {'content': "We do not share data without consent.", 'metadata': {'doc_source': 'Unknown Company'}}]

for idx, doc in enumerate(found_docs):
    print(f"Retrieved Document {idx} Content: {doc['content'][:200]}...")
    print(f"Document Source: {doc['metadata'].get('doc_source', 'Unknown Company')}")
    print(f"Metadata: {doc['metadata']}")

filtered_docs = [doc for doc in found_docs if doc['metadata'].get('doc_source').lower() == company_name.lower()]

if filtered_docs:
    generated_answer = "Here is the information we found on third-party data sharing:\n\n"
    for doc in filtered_docs:
        generated_answer += f"{doc['content']} (Source: {doc['metadata'].get('doc_source')})\n"
else:
    generated_answer = "No relevant information found for this query."

print("Generated Answer with Attribution:")
print(generated_answer)

"""### 3.3 The Test Data

You will want to test the system that you (will) have built. Below we give you a validation set that you could take as labeled data (imagine, your user personas would have had these questions and deemed the answers to be good). We also will give you a test set that only contains questions. (This is the set that we will use to get a feel for how well your RAG system corresponds to our Gold model).

Here are is the gold validation set and the test questions. **DO NOT CHANGE OR DELETE!!**
"""

validation = { 11856: {'company_name': 'meredith.com',
  'question': 'Will you share my data with a third party who does not involve in a practice?',
  'gold_answer': 'will not'},
 6439: {'company_name': 'barnesandnoble.com',
  'question': 'Do you allow others to get my information for providing customized feature? ',
  'gold_answer': 'This information will help us to better serve you and provide you with more personalized information and product offerings.'}}


test_questions = {548: {'company': 'honda.com', 'question': 'How do you use my data?'},
 803: {'company': 'honda.com',
       'question': 'Do you collect or use my information? If yes, then what '
                   'type?'}}

validation[11856]

test_questions[548]

"""### 3.4 Helper Functions"""

def get_context_mistral(context):
    global mistral_context
    mistral_context = context
    return context

def generate_response(validation_set, rag_chain):
    generated_answers = {}

    for question_id, question_data in validation_set.items():
        question = question_data['question']
        company_name = question_data['company_name']

        question_with_company = f"According to {company_name}'s privacy policy: {question}"

        response = rag_chain.invoke(question_with_company)

        generated_answers[question_id] = {
            "question": question,
            "generated_answer": response,
            "company_name": company_name
        }

    return generated_answers

def get_few_questions(validation, sample_size):
    question_keys = random.sample(list(validation.keys()), min(sample_size, len(validation)))
    sampled_questions = {k: validation[k] for k in question_keys}
    return sampled_questions

sample_size = 3
sampled_questions = get_few_questions(validation, sample_size)

print("The selected questions in this batch: ", sampled_questions)

def read_generated_answers(file_path):
    with open(file_path, 'r') as f:
        generated_answers = json.load(f)
    return generated_answers

simplified_prompt = """
Analyze the following privacy policy context and answer the user's question.
Here is the policy context: {context}.
Question: {question}.
Provide a concise, well-structured response based on this context.
"""

policy_prompt = ChatPromptTemplate.from_template(simplified_prompt)

def extract_company_name(question_data):
    return question_data.get('company_name', 'Unknown Company')

def format_question_with_company(question_data):
    company_name = extract_company_name(question_data)
    question = question_data['question']
    return f"Company: {company_name}. Question: {question}"

def run_rag_chain(question_data, retriever, policy_prompt, mistral_llm, output_parser):
    formatted_input = format_question_with_company(question_data)
    company_name = extract_company_name(question_data)

    context_documents = retriever.invoke(formatted_input)

    filtered_documents = [doc for doc in context_documents if doc.metadata.get('doc_source') == company_name]

    for idx, doc in enumerate(filtered_documents):
        print(f"Document chunk {idx} content: {doc.page_content[:200]}...")
        print(f"Metadata: {doc.metadata}")
        doc_link = doc.metadata.get('source_url', 'No URL available')
        print(f"Document source link: {doc_link}")

    formatted_context = format_docs(filtered_documents)

    prompt_input = {"context": formatted_context, "question": formatted_input}

    prompt = policy_prompt.invoke(prompt_input)

    response = mistral_llm.invoke(prompt)

    parsed_output = output_parser.parse(response)

    return parsed_output, filtered_documents

sample_question = {
    'company_name': 'meredith.com',
    'question': 'Will you share my data with a third party who does not involve in a practice?'
}

validation = { 11856: {'company_name': 'meredith.com',
  'question': 'Will you share my data with a third party who does not involve in a practice?',
  'gold_answer': 'will not'},
 6439: {'company_name': 'barnesandnoble.com',
  'question': 'Do you allow others to get my information for providing customized feature? ',
  'gold_answer': 'This information will help us to better serve you and provide you with more personalized information and product offerings.'}}


response, referenced_docs = run_rag_chain(sample_question, retriever, policy_prompt, mistral_llm_lc, output_parser)

print("Generated Response:", response)

for doc in referenced_docs:
    print(f"Document Chunk Content: {doc.page_content[:200]}...")
    print(f"Metadata: {doc.metadata}")

def extract_company_name(question_data):
    return question_data.get('company_name', 'Unknown Company')

def format_question_with_company(question_data):
    company_name = extract_company_name(question_data)
    question = question_data['question']
    return f"Company: {company_name}. Question: {question}"

def run_rag_chain(question_data, retriever, policy_prompt, mistral_llm, output_parser):
    formatted_input = format_question_with_company(question_data)
    company_name = extract_company_name(question_data)

    context_documents = retriever.invoke(formatted_input)

    filtered_documents = [doc for doc in context_documents if doc.metadata.get('doc_source') == company_name]

    for idx, doc in enumerate(filtered_documents):
        print(f"Document chunk {idx} content: {doc.page_content[:200]}...")
        doc_link = doc.metadata.get('source_url', 'No URL available')
        print(f"Document source link: {doc_link}")

    formatted_context = format_docs(filtered_documents)

    prompt_input = {"context": formatted_context, "question": formatted_input}

    prompt = policy_prompt.invoke(prompt_input)

    response = mistral_llm.invoke(prompt)

    parsed_output = output_parser.parse(response)

    return parsed_output, filtered_documents

def run_rag_chain_for_validation(validation_data, retriever, policy_prompt, mistral_llm, output_parser):

    results = {}

    for question_id, question_data in validation_data.items():
        print(f"Processing question ID: {question_id}")

        response, referenced_docs = run_rag_chain(question_data, retriever, policy_prompt, mistral_llm, output_parser)

        results[question_id] = {
            'generated_response': response,
            'referenced_docs': referenced_docs
        }

        print(f"Generated Response for question ID {question_id}: {response}")

        for doc in referenced_docs:
            print(f"Document Chunk Content: {doc.page_content[:200]}...")
            print(f"Metadata: {doc.metadata}")
            print(f"Document source link: {doc.metadata.get('source_url', 'No URL available')}")

    return results

validation = {
    11856: {
        'company_name': 'meredith.com',
        'question': 'Will you share my data with a third party who does not involve in a practice?',
        'gold_answer': 'will not'
    },
    6439: {
        'company_name': 'barnesandnoble.com',
        'question': 'Do you allow others to get my information for providing customized features?',
        'gold_answer': 'This information will help us to better serve you and provide you with more personalized information and product offerings.'
    }
}

validation_results = run_rag_chain_for_validation(validation, retriever, policy_prompt, mistral_llm_lc, output_parser)

print("\n=== Validation Results ===")
for question_id, result in validation_results.items():
    print(f"Question ID: {question_id}")
    print(f"Generated Response: {result['generated_response']}")
    print("Referenced Documents:")
    for doc in result['referenced_docs']:
        print(f"- Document Source: {doc.metadata.get('doc_source', 'Unknown')}")
        print(f"- Document Link: {doc.metadata.get('source_url', 'No URL available')}")
        print(f"- Document Content: {doc.page_content[:200]}...\n")

def run_rag_chain_for_validation(validation_data, retriever, policy_prompt, mistral_llm, output_parser):
    validation_results = []

    for question_id, question_data in validation_data.items():
        print(f"Processing question ID: {question_id}")

        response, referenced_docs = run_rag_chain(question_data, retriever, policy_prompt, mistral_llm, output_parser)

        context = "\n".join([doc.page_content for doc in referenced_docs])

        result = {
            'company_name': question_data['company_name'],
            'question': question_data['question'],
            'gold_answer': question_data.get('gold_answer', 'No gold answer available'),
            'context': context,
            'generated_answer': response
        }

        validation_results.append(result)

        print(f"Generated Response for question ID {question_id}: {response}")

        for doc in referenced_docs:
            print(f"Document Chunk Content: {doc.page_content[:200]}...")
            print(f"Metadata: {doc.metadata}")
            print(f"Document source link: {doc.metadata.get('source_url', 'No URL available')}")

    df = pd.DataFrame(validation_results)

    return df

validation = {
    11856: {
        'company_name': 'meredith.com',
        'question': 'Will you share my data with a third party who does not involve in a practice?',
        'gold_answer': 'will not'
    },
    6439: {
        'company_name': 'barnesandnoble.com',
        'question': 'Do you allow others to get my information for providing customized features?',
        'gold_answer': 'This information will help us to better serve you and provide you with more personalized information and product offerings.'
    }
}

validation_df = run_rag_chain_for_validation(validation, retriever, policy_prompt, mistral_llm_lc, output_parser)

print(validation_df)

validation_df.to_csv('validation_results.csv', index=False)

validation_df.head(1)

pd.set_option('display.max_colwidth', None)

validation_df.head(1)